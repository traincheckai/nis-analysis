{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3559772c-f0cb-44c0-abd5-ee6ad51ed74b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NIS Clinical Exploration Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db90e36-d386-472d-a775-ba5ac6c8ae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "import re\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time() # for timing the execution of the entire notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f097a-ddfb-4242-bed4-487359f50f0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37680c0b-c380-4213-8f67-729a52740646",
   "metadata": {},
   "source": [
    "Cervical spine example provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c37d337-d066-4214-a1eb-ca1ac12c0739",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb6905-6446-4b77-b5a0-3c7c31e2ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "patient_columns = ['AGE', # ensure AGE is first in list \n",
    "                   'FEMALE', \n",
    "                   'RACE',\n",
    "                   'YEAR',\n",
    "                   'PL_NCHS',\n",
    "                   'ZIPINC_QRTL',\n",
    "                   'HOSP_REGION', \n",
    "                   'PAY1', \n",
    "                   'TOTCHG', # -> {0,1} (<=avg_totchg, >avg_totchg)\n",
    "                   'LOS', # -> {1,2,3,4,5,6,7+} \n",
    "                   'PRDAY1', # -> {-1,0,1} (before, during, after) admission \n",
    "                   'DISPUNIFORM'\n",
    "                  ]\n",
    "\n",
    "# list of all NIS comorbidities\n",
    "comorbidity_columns = ['CMR_AIDS',\n",
    "                         'CMR_ALCOHOL',\n",
    "                         'CMR_AUTOIMMUNE',\n",
    "                         'CMR_CANCER_LYMPH',\n",
    "                         'CMR_CANCER_LEUK',\n",
    "                         'CMR_CANCER_METS',\n",
    "                         'CMR_CANCER_NSITU',\n",
    "                         'CMR_CANCER_SOLID',\n",
    "                         'CMR_DEMENTIA',\n",
    "                         'CMR_DEPRESS',\n",
    "                         'CMR_DIAB_UNCX',\n",
    "                         'CMR_DIAB_CX',\n",
    "                         'CMR_DRUG_ABUSE',\n",
    "                         'CMR_HTN_CX',\n",
    "                         'CMR_HTN_UNCX',\n",
    "                         'CMR_LUNG_CHRONIC',\n",
    "                         'CMR_OBESE',\n",
    "                         'CMR_PERIVASC',\n",
    "                         'CMR_THYROID_HYPO',\n",
    "                         'CMR_THYROID_OTH'\n",
    "                        ]\n",
    "\n",
    "\n",
    "# List comprehension for all 'I10_DX' columns (1 to 40) and primary procedure column \n",
    "i10_columns = [f'I10_PR{n}' for n in range(1, 25)] + [f'I10_DX{n}' for n in range(1, 40+1)]\n",
    "\n",
    "# Combine the two lists to form the complete list of selected columns\n",
    "selected_columns = patient_columns + comorbidity_columns + i10_columns \n",
    "\n",
    "nis_df = pd.read_parquet(\"nis-data.parquet\", columns=selected_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eff42a-bd13-44e1-be76-a54b23afd4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to check entry of diagnosis code in nis data \n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def get_nis_df(year):\n",
    "    nis_year = nis_df[nis_df['YEAR']==year]\n",
    "    cols = ['YEAR'] + [f'I10_DX{n}' for n in range(1, 40+1)]\n",
    "    nis_year = nis_year[cols]\n",
    "    return nis_year\n",
    "\n",
    "#data_2015 = get_nis_df(2015)\n",
    "#data_2016 = get_nis_df(2016)\n",
    "#data_2017 = get_nis_df(2017)\n",
    "#data_2018 = get_nis_df(2018)\n",
    "#data_2019 = get_nis_df(2019)\n",
    "\n",
    "diagnosis_cols = [f'I10_DX{n}' for n in range(1, 2)]\n",
    "\n",
    "# Function to check for similar values in given columns of a DataFrame\n",
    "def similar_value_in_columns(df, columns, target_value, similarity_threshold=0.8):\n",
    "    similar_values = []\n",
    "    for column in columns:\n",
    "        for index, value in df[column].items():  \n",
    "            if SequenceMatcher(None, str(value), target_value).ratio() >= similarity_threshold:\n",
    "                similar_values.append((index, column, value))\n",
    "    return similar_values\n",
    "\n",
    "#similar_values = similar_value_in_columns(data_2015, diagnosis_cols, 'M47.12')\n",
    "#similar_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7543c0a0-96a8-47ee-b37d-937613754d56",
   "metadata": {},
   "source": [
    "### Research Specific Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a914627b-cb97-4d27-bd6d-46431cff6a76",
   "metadata": {},
   "source": [
    "#### Diganosis Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc20b761-5e76-49f1-99c6-25f8ddf7b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the original codes\n",
    "base_total_knee_codes = ['0SRC06', '0SRC07', '0SRC0J', '0SRC0K', '0SRD06', '0SRD07', '0SRD0J', '0SRD0K']\n",
    "#base_partial_knee_codes = ['0SRC0L', '0SRC0M', '0SRC0N', '0SRD0L', '0SRD0M', '0SRD0N']\n",
    "\n",
    "# Create modified versions with a period after the first three characters\n",
    "modified_total_knee = [code[:3] + '.' + code[3:] for code in base_total_knee_codes]\n",
    "#modified_partial_knee = [code[:3] + '.' + code[3:] for code in base_partial_knee_codes]\n",
    "\n",
    "# Concatenate the base and modified lists\n",
    "total_knee = base_total_knee_codes + modified_total_knee\n",
    "#partial_knee = base_partial_knee_codes + modified_partial_knee\n",
    "\n",
    "print(\"Total Knee Codes:\", total_knee)\n",
    "#print(\"Partial Knee Codes:\", partial_knee)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c89a8-05a5-4d52-beae-dfa81e6c4992",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def determine_procedure_type(code):\n",
    "    if code is None:\n",
    "        return None\n",
    "    if any(code.startswith(prefix) for prefix in total_knee):\n",
    "        return 'total_knee'\n",
    "    #if any(code.startswith(prefix) for prefix in partial_knee):\n",
    "    #    return 'partial_knee'\n",
    "    return None\n",
    "\n",
    "# Apply the function to the entire Series instead of row by row\n",
    "nis_df['Procedure_Type'] = nis_df['I10_PR1'].apply(determine_procedure_type)\n",
    "\n",
    "# Calculate and print the frequencies\n",
    "procedure_type_freq = nis_df['Procedure_Type'].value_counts()\n",
    "print(procedure_type_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804f183-0925-4317-a7a2-edd130362044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# primary diagnosis of interest (with variant for 2016)\n",
    "diagnosis_code = 'M17.0' \n",
    "diagnosis_codes = ['M17.0', 'M170']\n",
    "\n",
    "# Combine the original and modified lists\n",
    "#procedure_codes = [\"total_knee\", \"partial_knee\"]\n",
    "procedure_codes = [\"total_knee\"]\n",
    "\n",
    "# List of complication code prefixes\n",
    "complication_code_prefixes = ['M96', 'G97', 'I97', \"M66\", 'T81', 'T82', 'T84', 'T85', 'T88'] \n",
    "\n",
    "# variables needing to be condensed into smaller bins\n",
    "condensed = ['AGE', 'TOTCHG', 'LOS', 'PRDAY1'] # use an empty list if no condensing is required\n",
    "\n",
    "# Names\n",
    "researcher = '' \n",
    "data_folder_name = '_data' # default\n",
    "table_1_name = \"table_1.csv\" # default\n",
    "table_2_name = \"table_2.csv\" # default\n",
    "table_3_name = \"table_3.csv\" # default\n",
    "table_4_name = \"table_4.csv\" # default\n",
    "table_5_name = \"table_5.csv\" # default\n",
    "\n",
    "# Ensure the directory exists/ create folder \n",
    "os.makedirs(data_folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51901a37-d49b-4242-b285-372924b6081d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Table 3 Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1203b58-f295-4df6-a265-f2a8604758ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_var = 'complications'\n",
    "\n",
    "demographic_independent_variables = sorted(patient_columns)\n",
    "#demographic_reference_variables.keys = demographic_independent_variables\n",
    "#demographic_reference_variables.values = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b44efe1-0719-41db-948a-d43a9886591b",
   "metadata": {},
   "source": [
    "### Table 4 Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3374c5-1e93-443f-816d-06f29e951ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_4_adjusted = ['FEMALE', 'AGE', 'RACE', 'ZIPINC_QRTL'] # demographic variables to adjust for "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba230ab3-761f-4751-bc2e-ef7848382f0e",
   "metadata": {},
   "source": [
    "### Table 5 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6eb499-5f81-43be-98e9-7ba0f91cce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compartment\n",
    "base_bilateral = ['0SR.C06', '0SRC.07', '0SR.C0J', '0SR.C0K', '0SR.D06', '0SR.D07', '0SR.D0J', '0SR.D0K', '0SRC06', '0SRC07', '0SRC0J', '0SRC0K', '0SRD06', '0SRD07', '0SRD0J', '0SRD0K'] \n",
    "base_unicondylar = ['0SR.C0L', '0SR.C0M', '0SR.D0L', '0SR.D0M','0SRC0L', '0SRC0M', '0SRD0L', '0SRD0M']\n",
    "base_patellofemoral = ['0SR.C0N', '0SR.D0N', '0SRC0N', '0SRD0N']\n",
    "\n",
    "# 2. Cementation status\n",
    "base_cemented = ['0SRC069', '0SRC0J9', '0SRC0L9', '0SRC0M9', '0SRC0N9', '0SRD069', '0SRD0J9', '0SRD0L9', '0SRD0M9', '0SRD0N9','0SR.C069', '0SR.C0J9', '0SR.C0L9', '0SR.C0M9', '0SR.C0N9', '0SR.D069', '0SR.D0J9', '0SR.D0L9', '0SR.D0M9', '0SR.D0N9']\n",
    "base_uncemented = ['0SR.C06A', '0SR.C0JA', '0SR.C0LA', '0SR.C0MA', '0SR.C0NA', '0SR.D06A', '0SR.D0JA', '0SR.D0LA', '0SR.D0MA', '0SR.D0NA']\n",
    "\n",
    "# 3. Tissue substitute\n",
    "base_synthetic = ['0SRC06A', '0SRC0JA', '0SRC0LA', '0SRC0MA', '0SRC0NA', '0SRD06A', '0SRD0JA', '0SRD0LA', '0SRD0MA', '0SRD0NA','0SR.C06', '0SR.C0J', '0SR.C0L', '0SR.C0M', '0SR.C0N', '0SR.D06', '0SR.D0J', '0SR.D0L', '0SR.D0M', '0SR.D0N']\n",
    "base_autologous = ['0SRC07', '0SRD07','0SR.C07', '0SR.D07']\n",
    "base_non_autologous = ['0SRC0K', '0SRD0K','0SR.C0K', '0SR.D0K']\n",
    "\n",
    "# 4. Articulating spacer\n",
    "base_articulating_spacer_yes = ['0SRC0E', '0SRD0E','0SR.C0E', '0SR.D0E']\n",
    "\n",
    "# Function to remove '.' from codes and add them to the base list\n",
    "def modify_and_combine(base_list):\n",
    "    modified_list = [code.replace('.', '') for code in base_list]\n",
    "    return base_list + modified_list\n",
    "\n",
    "# 1. Compartment\n",
    "bilateral = modify_and_combine(base_bilateral)\n",
    "unicondylar = modify_and_combine(base_unicondylar)\n",
    "patellofemoral = modify_and_combine(base_patellofemoral)\n",
    "\n",
    "# 2. Cementation status\n",
    "cemented = modify_and_combine(base_cemented)\n",
    "uncemented = modify_and_combine(base_uncemented)\n",
    "\n",
    "# 3. Tissue substitute\n",
    "synthetic = modify_and_combine(base_synthetic)\n",
    "autologous = modify_and_combine(base_autologous)\n",
    "non_autologous = modify_and_combine(base_non_autologous)\n",
    "\n",
    "# 4. Articulating spacer\n",
    "articulating_spacer_yes = modify_and_combine(base_articulating_spacer_yes)\n",
    "\n",
    "surgical_variables = ['bilateral',\n",
    "                       'unicondylar',\n",
    "                       'patellofemoral'\n",
    "                      ]\n",
    "\n",
    "surgical_variables_2 = ['cemented',\n",
    "                         'uncemented'\n",
    "                        ]\n",
    "\n",
    "surgical_variables_3 = ['synthetic',\n",
    "                         'autologous',\n",
    "                        'non_autologous'\n",
    "                        ]\n",
    "\n",
    "surgical_variables_4 = ['articulating_spacer_yes'\n",
    "                        ]\n",
    "\n",
    "surgical_independent_variables_with_reference = surgical_variables + surgical_variables_2 + surgical_variables_3 + surgical_variables_4\n",
    "surgical_independent_variables = surgical_independent_variables_with_reference\n",
    "\n",
    "table_5_adjusted = ['comorbidity', 'FEMALE', 'AGE', 'RACE', 'ZIPINC_QRTL'] # comorbidity + demographic variables to adjust for "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ff7b1-25c4-4b0c-a4d5-1340480162ab",
   "metadata": {},
   "source": [
    "See [link](https://hcup-us.ahrq.gov/db/nation/nis/nisdde.jsp) for full data description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf5de28-6703-4988-b5ef-1c1dd97a0e04",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2fdab1-1949-4a00-ae4b-98fc3d9f8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_snapshot(df):\n",
    "    \"\"\"\n",
    "    Creates a deep copy of the provided DataFrame to be used as a snapshot for comparison.\n",
    "    \"\"\"\n",
    "    return copy.deepcopy(df)\n",
    "\n",
    "def assert_unchanged(original_df_snapshot, current_df):\n",
    "    \"\"\"\n",
    "    Asserts that the current DataFrame is identical to the original snapshot.\n",
    "    \"\"\"\n",
    "    msg=\"DataFrame has been altered\"\n",
    "    pd.testing.assert_frame_equal(original_df_snapshot, current_df, \n",
    "                                  check_dtype=True, \n",
    "                                  check_exact=True, \n",
    "                                  check_like=True, \n",
    "                                  check_datetimelike_compat=True, \n",
    "                                  check_categorical=True, \n",
    "                                  rtol=0, \n",
    "                                  atol=0, \n",
    "                                  obj=msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8dedfd-7d4c-4b77-9233-5a762d245fac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Diagnosis Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad5ab4-19bb-4b38-a7d8-8040fa6dc20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering based on primary diagnosis (This is the universe of discharges that we are concerned with)\n",
    "#diagnosis_df = nis_df[nis_df['I10_DX1'] == diagnosis_code].copy()\n",
    "# Filter the DataFrame for rows where 'I10_DX1' matches any of the codes in the list\n",
    "diagnosis_df = nis_df[nis_df['I10_DX1'].isin(diagnosis_codes)].copy()\n",
    "\n",
    "diagnosis_df = diagnosis_df[diagnosis_df['AGE'] >= 35]\n",
    "\n",
    "diagnosis_df_snapshot = create_df_snapshot(diagnosis_df) # record instantiation\n",
    "diagnosis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e637f-dc68-4f31-8d9e-0dcc28a8b28a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Procedure Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c83a00-e0af-4ede-b830-5afe66668c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#procedure_df = diagnosis_df[diagnosis_df['I10_PR1'].isin(procedure_codes)]\n",
    "# fill nan values with empty string before searching to avoid error\n",
    "procedure_df = diagnosis_df[diagnosis_df['Procedure_Type'].fillna('').isin(procedure_codes)]\n",
    "\n",
    "# Dictionary to hold the dataframes for each procedure code (if needed)\n",
    "procedure_dfs = {}\n",
    "\n",
    "# Loop through each code in the procedure_codes list\n",
    "for i, code in enumerate(procedure_codes, start=1):\n",
    "    # Filter diagnosis_df for the current procedure code and assign to the dictionary\n",
    "    procedure_dfs[f'procedure{i}_df'] = diagnosis_df[diagnosis_df['Procedure_Type'] == code]\n",
    "    \n",
    "# Assign each DataFrame in procedure_dfs to a global variable with the same name\n",
    "for df_name, df in procedure_dfs.items():\n",
    "    globals()[df_name] = df\n",
    "\n",
    "print(f\"defined: {list(procedure_dfs.keys())}\") # Now, procedure1_df, procedure2_df, ... are your DataFrames\n",
    "\n",
    "# complement of procedure_df wrt diagnosis_df\n",
    "non_procedure_df = diagnosis_df.drop(procedure_df.index) \n",
    "\n",
    "def add_complication_column(df):\n",
    "    df = df.copy()\n",
    "    # List of columns to check for complication codes\n",
    "    dx_columns = [f'I10_DX{n}' for n in range(2, 41)]\n",
    "    \n",
    "    # Function to check if any cell value starts with any of the complication prefixes\n",
    "    def has_complication(row):\n",
    "        for prefix in complication_code_prefixes:\n",
    "            if any(row[col].startswith(prefix) for col in dx_columns if pd.notna(row[col])):\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    # Apply the function to each row\n",
    "    df['complications'] = df[dx_columns].apply(has_complication, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# add complications column to procedure_df (will be used later for table_3)\n",
    "procedure_df = add_complication_column(procedure_df)\n",
    "procedure_df_snapshot = create_df_snapshot(procedure_df) # record instantiation\n",
    "procedure_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3cb405-4243-4ca9-8192-b54966627f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedure1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5acf4d-3516-4243-a465-480ec87c1623",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"For discharges with primary diagnosis {diagnosis_code}:\")\n",
    "print(\"\")\n",
    "\n",
    "avg_age = round(diagnosis_df[\"AGE\"].mean())\n",
    "std_age = round(diagnosis_df[\"AGE\"].std())\n",
    "print(f\"Average age: {avg_age} years (standard deviation age: {std_age} years)\")\n",
    "\n",
    "avg_los = round(diagnosis_df[\"LOS\"].mean())\n",
    "print(f\"Average length of stay: {avg_los} days\")\n",
    "\n",
    "avg_totchg = round(diagnosis_df[\"TOTCHG\"].mean())\n",
    "formatted_avg_totchg = f\"${avg_totchg:,.0f} USD\"\n",
    "print(f\"Average total charge: {formatted_avg_totchg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e5d5bd-041d-4016-aaa4-847bfe2f3a29",
   "metadata": {},
   "source": [
    "# Table 1: Table of Patient Characterstics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac3368-0eb0-42f0-af81-d5254bf50da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_unchanged(procedure_df_snapshot, procedure_df)\n",
    "assert_unchanged(diagnosis_df_snapshot, diagnosis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaf8d4c-29e2-43ff-a2fb-b704e8352474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_patient_characteristics_table():\n",
    "    global diagnosis_df, procedure_df, non_procedure_df\n",
    "\n",
    "    # Ensure the dataframes are copied\n",
    "    temp_proc_df = procedure_df.copy()\n",
    "    temp_non_proc_df = non_procedure_df.copy()\n",
    "\n",
    "    # Define new age bins and labels\n",
    "    age_bins = [35, 50, 75, float('inf')]\n",
    "    age_labels = ['35-49', '50-74', '75+']\n",
    "\n",
    "    # Convert AGE to numeric and handle non-numeric values\n",
    "    #temp_proc_df['AGE'] = pd.to_numeric(temp_proc_df['AGE'], errors='coerce')\n",
    "    #temp_non_proc_df['AGE'] = pd.to_numeric(temp_non_proc_df['AGE'], errors='coerce')\n",
    "\n",
    "    # Bin the AGE data\n",
    "    temp_proc_df['AGE_bin'] = pd.cut(temp_proc_df['AGE'], bins=age_bins, labels=age_labels, right=False)\n",
    "    temp_non_proc_df['AGE_bin'] = pd.cut(temp_non_proc_df['AGE'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "    # Initialize a table with columns 'procedure' and 'non-procedure'\n",
    "    table = pd.DataFrame(columns=['procedure', 'non-procedure']).astype('object')\n",
    "\n",
    "    # Total count\n",
    "    table.loc['total count', 'procedure'] = f\"{len(temp_proc_df)} ({len(temp_proc_df)/len(diagnosis_df):.2%})\"\n",
    "    table.loc['total count', 'non-procedure'] = f\"{len(temp_non_proc_df)} ({len(temp_non_proc_df)/len(diagnosis_df):.2%})\"\n",
    "\n",
    "    # Age mean and standard deviation\n",
    "    age_mean_std_procedure = f\"{temp_proc_df['AGE'].mean():.2f}, {temp_proc_df['AGE'].std():.2f}\"\n",
    "    age_mean_std_nonprocedure = f\"{temp_non_proc_df['AGE'].mean():.2f}, {temp_non_proc_df['AGE'].std():.2f}\"\n",
    "    table.loc['AGE (mean, std)', 'procedure'] = age_mean_std_procedure\n",
    "    table.loc['AGE (mean, std)', 'non-procedure'] = age_mean_std_nonprocedure\n",
    "\n",
    "    # Age bins count\n",
    "    for label in age_labels:\n",
    "        count_procedure = temp_proc_df[temp_proc_df['AGE_bin'] == label].shape[0]\n",
    "        count_non_procedure = temp_non_proc_df[temp_non_proc_df['AGE_bin'] == label].shape[0]\n",
    "        table.loc[f'AGE {label}', 'procedure'] = f\"{count_procedure} ({count_procedure/len(temp_proc_df):.2%})\"\n",
    "        table.loc[f'AGE {label}', 'non-procedure'] = f\"{count_non_procedure} ({count_non_procedure/len(temp_non_proc_df):.2%})\"\n",
    "    \n",
    "    print(temp_proc_df)\n",
    "    return table\n",
    "\n",
    "\n",
    "def create_non_condensed_patient_characteristics_table():\n",
    "    characteristics = [col for col in patient_columns[1:] if col not in condensed]\n",
    "\n",
    "    table = init_patient_characteristics_table()\n",
    "    for char in characteristics:\n",
    "        for value in diagnosis_df[char].unique():\n",
    "            count_procedure = procedure_df[procedure_df[char] == value].shape[0]\n",
    "            count_non_procedure = non_procedure_df[non_procedure_df[char] == value].shape[0]\n",
    "\n",
    "            if count_procedure > 0 and count_non_procedure > 0:\n",
    "                table.loc[f'{char}_{value}', 'procedure'] = f\"{count_procedure} ({count_procedure/len(procedure_df):.2%})\"\n",
    "                table.loc[f'{char}_{value}', 'non-procedure'] = f\"{count_non_procedure} ({count_non_procedure/len(non_procedure_df):.2%})\"\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def add_LOS(table):\n",
    "    # Temporary DataFrames with LOS binning\n",
    "    temp_proc_df = procedure_df.copy()\n",
    "    temp_non_proc_df = non_procedure_df.copy()\n",
    "\n",
    "    # Define the bins and labels for the LOS\n",
    "    bins = [0, 1, 2, 3, 4, 5, 6, 7, float('inf')]\n",
    "    labels = ['0', '1', '2', '3', '4', '5', '6', '7+']\n",
    "\n",
    "    # Bin the LOS data\n",
    "    temp_proc_df['LOS_bin'] = pd.cut(temp_proc_df['LOS'], bins=bins, labels=labels, right=False)\n",
    "    temp_non_proc_df['LOS_bin'] = pd.cut(temp_non_proc_df['LOS'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # Calculate counts and proportions for each bin\n",
    "    for label in labels:\n",
    "        count_procedure = temp_proc_df[temp_proc_df['LOS_bin'] == label].shape[0]\n",
    "        count_non_procedure = temp_non_proc_df[temp_non_proc_df['LOS_bin'] == label].shape[0]\n",
    "\n",
    "        if count_procedure > 0 and count_non_procedure > 0:\n",
    "            table.loc[f'LOS_{label}', 'procedure'] = f\"{count_procedure} ({count_procedure/len(temp_proc_df):.2%})\"\n",
    "            table.loc[f'LOS_{label}', 'non-procedure'] = f\"{count_non_procedure} ({count_non_procedure/len(temp_non_proc_df):.2%})\"\n",
    "\n",
    "    return table\n",
    "\n",
    "def add_LOS_table_3(table):\n",
    "    local_df = procedure_df.copy()  # Work on a copy\n",
    "    bins = [0, 1, 2, 3, 4, 5, 6, float('inf')]\n",
    "    labels = ['0', '1', '2', '3', '4', '5', '6', '7+']\n",
    "    local_df['LOS_bin'] = pd.cut(local_df['LOS'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    for label in labels:\n",
    "        subset_df = local_df[local_df['LOS_bin'] == label]\n",
    "        count_procedure = subset_df.shape[0]\n",
    "        count_complications = subset_df['complications'].sum()\n",
    "\n",
    "        table.loc[f'LOS_{label}', 'procedure'] = f\"{count_procedure} ({count_procedure/len(local_df):.2%})\"\n",
    "        table.loc[f'LOS_{label}', 'complication'] = f\"{count_complications} ({count_complications/count_procedure:.2%})\" if count_procedure > 0 else '0 (0.00%)'\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def add_PRDAY1(table):\n",
    "    # Temporary DataFrames with PRDAY1 categorization\n",
    "    temp_proc_df = procedure_df.copy()\n",
    "    temp_non_proc_df = non_procedure_df.copy()\n",
    "\n",
    "    # Define a function to categorize PRDAY1 values\n",
    "    def categorize_prday1(value):\n",
    "        if value >= -4 and value <= -1:\n",
    "            return 'before'\n",
    "        elif value == 0:\n",
    "            return 'during'\n",
    "        elif value >= 1: # \n",
    "            return 'after'\n",
    "        else:\n",
    "            return 'other'\n",
    "\n",
    "    # Apply categorization to PRDAY1\n",
    "    temp_proc_df['PRDAY1_cat'] = temp_proc_df['PRDAY1'].apply(categorize_prday1)\n",
    "    temp_non_proc_df['PRDAY1_cat'] = temp_non_proc_df['PRDAY1'].apply(categorize_prday1)\n",
    "\n",
    "    # Calculate counts and proportions for each category\n",
    "    for category in ['before', 'during', 'after', 'other']:\n",
    "        count_procedure = temp_proc_df[temp_proc_df['PRDAY1_cat'] == category].shape[0]\n",
    "        count_non_procedure = temp_non_proc_df[temp_non_proc_df['PRDAY1_cat'] == category].shape[0]\n",
    "\n",
    "        if count_procedure > 0 and count_non_procedure > 0:\n",
    "            table.loc[f'PRDAY1_{category}', 'procedure'] = f\"{count_procedure} ({count_procedure/len(temp_proc_df):.2%})\"\n",
    "            table.loc[f'PRDAY1_{category}', 'non-procedure'] = f\"{count_non_procedure} ({count_non_procedure/len(temp_non_proc_df):.2%})\"\n",
    "\n",
    "    return table\n",
    "\n",
    "def add_TOTCHG(table):\n",
    "    # Temporary DataFrames with TOTCHG categorization\n",
    "    temp_proc_df = procedure_df.copy()\n",
    "    temp_non_proc_df = non_procedure_df.copy()\n",
    "\n",
    "    # Categorize TOTCHG based on the condition\n",
    "    temp_proc_df['TOTCHG_cat'] = (temp_proc_df['TOTCHG'] > avg_totchg).astype(int)\n",
    "    temp_non_proc_df['TOTCHG_cat'] = (temp_non_proc_df['TOTCHG'] > avg_totchg).astype(int)\n",
    "\n",
    "    # Calculate counts and proportions for each category (0 and 1)\n",
    "    for category in [0, 1]:\n",
    "        count_procedure = temp_proc_df[temp_proc_df['TOTCHG_cat'] == category].shape[0]\n",
    "        count_non_procedure = temp_non_proc_df[temp_non_proc_df['TOTCHG_cat'] == category].shape[0]\n",
    "\n",
    "        if count_procedure > 0 and count_non_procedure > 0:\n",
    "            table.loc[f'TOTCHG_{category}', 'procedure'] = f\"{count_procedure} ({count_procedure/len(temp_proc_df):.2%})\"\n",
    "            table.loc[f'TOTCHG_{category}', 'non-procedure'] = f\"{count_non_procedure} ({count_non_procedure/len(temp_non_proc_df):.2%})\"\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def create_patient_characteristics_table():\n",
    "    # Initialize the table\n",
    "    table = create_non_condensed_patient_characteristics_table() # calls init_patient_characteristics_table\n",
    "\n",
    "    # Add condensed variables based on the condensed list\n",
    "    for condensed_variable in condensed:\n",
    "        if condensed_variable == 'TOTCHG':\n",
    "            table = add_TOTCHG(table)\n",
    "        elif condensed_variable == 'LOS':\n",
    "            table = add_LOS(table)\n",
    "        elif condensed_variable == 'PRDAY1':\n",
    "            table = add_PRDAY1(table)\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7b5cd5-ca1f-4f6f-bfda-50441e829c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_percentages_sum_to_100(table, tolerance = 0.1):\n",
    "    # Function to extract percentage value from a string\n",
    "    def extract_percentage(s):\n",
    "        match = re.search(r\"(\\d+\\.\\d+)%\", s)\n",
    "        return float(match.group(1)) if match else 0\n",
    "\n",
    "    # List to store error messages\n",
    "    errors = []\n",
    "\n",
    "    # Check 'total count' row first\n",
    "    if 'total count' in table.index:\n",
    "        total_count_procedure = extract_percentage(table.loc['total count', 'procedure'])\n",
    "        total_count_non_procedure = extract_percentage(table.loc['total count', 'non-procedure'])\n",
    "        if not (97 <= (total_count_procedure + total_count_non_procedure) <= 100.1):  # Allowing a tiny margin for floating-point arithmetic\n",
    "            errors.append(\"Total counts in 'procedure' and 'non-procedure' do not sum to approximately 100%\")\n",
    "\n",
    "    # Initialize a dictionary to hold the sum of percentages for each variable and category\n",
    "    percentage_sums = {}\n",
    "\n",
    "    # Iterate through each cell in the table to sum the percentages\n",
    "    for index, row in table.iterrows():\n",
    "        # Skip 'total count' and 'AGE (mean, std)' rows\n",
    "        if index in ['total count', 'AGE (mean, std)']:\n",
    "            continue\n",
    "\n",
    "        # Extract variable name (e.g., 'FEMALE' from 'FEMALE_0.0')\n",
    "        var_name = index.split('_')[0]\n",
    "\n",
    "        # Sum percentages for each variable across its categories\n",
    "        for col in table.columns:\n",
    "            val = extract_percentage(row[col])\n",
    "            if var_name not in percentage_sums:\n",
    "                percentage_sums[var_name] = {col: 0}\n",
    "            if col not in percentage_sums[var_name]:\n",
    "                percentage_sums[var_name][col] = 0\n",
    "            percentage_sums[var_name][col] += val\n",
    "\n",
    "    # Check if the sums are approximately 100% for each variable in each column\n",
    "    #tolerance = 0.1  # e.g., 0.1% tolerance\n",
    "    for var, cols in percentage_sums.items():\n",
    "        for col, total in cols.items():\n",
    "            if not (100 - tolerance <= total <= 100 + tolerance):\n",
    "                errors.append(f\"Variable '{var}' in column '{col}' does not sum to approximately 100% (Sum: {total}%)\")\n",
    "\n",
    "    # Check if any errors were found\n",
    "    if errors:\n",
    "        return False, \"\\n\".join(errors)\n",
    "    else:\n",
    "        return True, \"All checks passed: Total counts and variables sum to approximately 100% per column\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc4b215-e884-48a2-9221-a483e8c39231",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_1 = create_patient_characteristics_table()\n",
    "table_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5399d6-3029-42dc-af85-b839ee617a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result, message = check_percentages_sum_to_100(table_1, 0.1)\n",
    "print(result, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b334d5b9-2adc-42fd-8673-d974387065ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame as a CSV file\n",
    "table_1.to_csv(f\"{data_folder_name}/{researcher}_{table_1_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e65d77a-84af-4398-a58d-4d305b96aeaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Table 2: Complication Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34bc956-de09-41db-a1e9-b5ee6c15efe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_unchanged(procedure_df_snapshot, procedure_df)\n",
    "assert_unchanged(diagnosis_df_snapshot, diagnosis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659815cb-6519-453d-86c1-581cb0c81561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_complication_codes(dataframe=diagnosis_df, start_col=2, end_col=40):\n",
    "    # Generate the column names based on the provided start and end columns\n",
    "    column_names = [f'I10_DX{i}' for i in range(start_col, end_col + 1)]\n",
    "    # Initialize an empty set to store unique codes across all columns\n",
    "    unique_codes_set = set()\n",
    "    \n",
    "    # Iterate over each diagnosis column and find unique codes that start with the given prefix\n",
    "    for column in column_names:\n",
    "        unique_values = dataframe[column].dropna().unique()\n",
    "        # Update the unique codes set with codes that start with the prefix\n",
    "        for prefix in complication_code_prefixes:\n",
    "            unique_codes_set.update(code for code in unique_values if str(code).startswith(prefix))\n",
    "    \n",
    "    # Return the unique codes as a sorted list\n",
    "    return sorted(unique_codes_set)\n",
    "\n",
    "# Example of how to use the refactored function:\n",
    "complications_codes = find_complication_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f536120a-3753-448b-8a70-01dd34b98f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedure1_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff980f3-2bcd-47e9-92f6-a6daf7ddb5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complication_counts(df, name):\n",
    "    dx_columns = [f'I10_DX{n}' for n in range(2, 41)]\n",
    "    complication_counts_per_prefix = {}\n",
    "\n",
    "    df_length = len(df)\n",
    "    \n",
    "    for prefix in complication_code_prefixes:\n",
    "        count = df[dx_columns].apply(lambda row: any(row.str.startswith(prefix, na=False)), axis=1).sum()\n",
    "        if count >= 10:\n",
    "            proportion = 100 * count / df_length if df_length > 0 else 0\n",
    "            complication_counts_per_prefix[prefix] = (count, round(proportion, 2))\n",
    "\n",
    "    total_complications = sum(count for count, _ in complication_counts_per_prefix.values())\n",
    "    complication_counts_per_prefix['Total'] = (total_complications, round(100 * total_complications / df_length, 2) if df_length > 0 else 0)\n",
    "\n",
    "    return complication_counts_per_prefix\n",
    "\n",
    "\n",
    "def get_complications_table():\n",
    "    # Assuming complication_code_prefixes is defined globally\n",
    "    complication_summary_df = pd.DataFrame()\n",
    "\n",
    "    diagnosis_data = get_complication_counts(diagnosis_df, \"diagnosis\")\n",
    "    procedure_data = get_complication_counts(procedure_df, \"procedure\")\n",
    "    \n",
    "    # Determine the common complications across both datasets\n",
    "    common_complications = set(diagnosis_data).intersection(set(procedure_data))\n",
    "    complications_table_index = list(common_complications) + ['Total']\n",
    "\n",
    "    complication_summary_df['Diagnosis'] = [f\"{diagnosis_data[prefix][0]} ({diagnosis_data[prefix][1]}%)\" for prefix in complications_table_index]\n",
    "    complication_summary_df['Procedure'] = [f\"{procedure_data[prefix][0]} ({procedure_data[prefix][1]}%)\" for prefix in complications_table_index]\n",
    "\n",
    "    if 'procedure_dfs' in globals() and isinstance(procedure_dfs, dict):\n",
    "        for df_name, df in procedure_dfs.items():\n",
    "            df_data = get_complication_counts(df, df_name)\n",
    "            complication_summary_df[df_name] = [f\"{df_data[prefix][0]} ({df_data[prefix][1]}%)\" if prefix in df_data else \"0 (0%)\" for prefix in complications_table_index]\n",
    "\n",
    "    complication_summary_df.index = complications_table_index\n",
    "    return complication_summary_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cfdfd5-d9b9-4f6a-a40e-5e3d6293a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_2 = get_complications_table()\n",
    "table_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195585cc-8f9b-448a-816f-8801286e384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_count(value):\n",
    "    # Extracts numerical count from the cell value\n",
    "    match = re.match(r\"(\\d+)\", value)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "def test_complication_counts(table):\n",
    "    # Summing counts from all 'procedureX_df' columns, excluding 'Procedure'\n",
    "    procedure_totals = sum([sum(table[col].apply(extract_count)) for col in table.columns if col.startswith('proc') and col != 'Procedure'])\n",
    "\n",
    "    # Extract total counts for Procedure and Diagnosis\n",
    "    procedure_total_count = extract_count(table.loc['Total', 'Procedure'])\n",
    "    diagnosis_total_count = extract_count(table.loc['Total', 'Diagnosis'])\n",
    "\n",
    "    # Check if the sums are as expected\n",
    "    assert procedure_totals == procedure_total_count, \"Sum of procedure counts does not match total procedure count\"\n",
    "    assert procedure_total_count < diagnosis_total_count, \"Total procedure count is not less than total diagnosis count\"\n",
    "\n",
    "    return \"Test passed\"\n",
    "\n",
    "# Assuming 'table' is your DataFrame\n",
    "# test_complication_counts(table)\n",
    "\n",
    "\n",
    "#test_complication_counts(table_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43db8ea6-3768-4397-b641-d3650feda459",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_2.to_csv(f\"{data_folder_name}/{researcher}_{table_2_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f506c6-5f81-4478-90b6-4e0cd50028ed",
   "metadata": {},
   "source": [
    "# Table 3: Demographic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3328caf5-2d8a-46bb-a114-3f34c549fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_unchanged(procedure_df_snapshot, procedure_df)\n",
    "assert_unchanged(diagnosis_df_snapshot, diagnosis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b135b16-d2ef-4099-9367-aabd36f46524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_AGE_table_3(table):\n",
    "    local_df = procedure_df.copy()  # Work on a copy\n",
    "\n",
    "    # Define new age bins and labels\n",
    "    bins = [35,50, 75, float('inf')]\n",
    "    labels = ['35-49','50-74', '75+']\n",
    "    local_df['AGE_bin'] = pd.cut(local_df['AGE'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    for label in labels:\n",
    "        subset_df = local_df[local_df['AGE_bin'] == label]\n",
    "        count_procedure = subset_df.shape[0]\n",
    "        count_complications = subset_df['complications'].sum()\n",
    "\n",
    "        # Calculate and format the statistics\n",
    "        proc_stat = f\"{count_procedure} ({count_procedure/len(local_df):.2%})\"\n",
    "        comp_stat = f\"{count_complications} ({count_complications/count_procedure:.2%})\" if count_procedure > 0 else '0 (0.00%)'\n",
    "\n",
    "        # Add statistics to the table\n",
    "        table.loc[f'AGE_{label}', 'procedure'] = proc_stat\n",
    "        table.loc[f'AGE_{label}', 'complication'] = comp_stat\n",
    "\n",
    "    return table\n",
    "\n",
    "def add_LOS_table_3(table):\n",
    "    local_df = procedure_df.copy()  # Work on a copy\n",
    "    bins = [0, 1, 2, 3, 4, 5, 6, 7, float('inf')]\n",
    "    labels = ['0', '1', '2', '3', '4', '5', '6', '7+']\n",
    "    local_df['LOS_bin'] = pd.cut(local_df['LOS'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    for label in labels:\n",
    "        subset_df = local_df[local_df['LOS_bin'] == label]\n",
    "        count_procedure = subset_df.shape[0]\n",
    "        count_complications = subset_df['complications'].sum()\n",
    "\n",
    "        table.loc[f'LOS_{label}', 'procedure'] = f\"{count_procedure} ({count_procedure/len(local_df):.2%})\"\n",
    "        table.loc[f'LOS_{label}', 'complication'] = f\"{count_complications} ({count_complications/count_procedure:.2%})\" if count_procedure > 0 else '0 (0.00%)'\n",
    "\n",
    "    return table\n",
    "\n",
    "def add_PRDAY1_table_3(table):\n",
    "    local_df = procedure_df.copy()  # Work on a copy\n",
    "    def categorize_prday1(value):\n",
    "        if value >= -4 and value <= -1:\n",
    "            return 'before'\n",
    "        elif value == 0:\n",
    "            return 'during'\n",
    "        elif value >= 1:\n",
    "            return 'after'\n",
    "        else:\n",
    "            return 'other'\n",
    "\n",
    "    local_df['PRDAY1_cat'] = local_df['PRDAY1'].apply(categorize_prday1)\n",
    "\n",
    "    for category in ['before', 'during', 'after', 'other']:\n",
    "        subset_df = local_df[local_df['PRDAY1_cat'] == category]\n",
    "        count_procedure = subset_df.shape[0]\n",
    "        count_complications = subset_df['complications'].sum()\n",
    "\n",
    "        table.loc[f'PRDAY1_{category}', 'procedure'] = f\"{count_procedure} ({count_procedure/len(local_df):.2%})\"\n",
    "        table.loc[f'PRDAY1_{category}', 'complication'] = f\"{count_complications} ({count_complications/count_procedure:.2%})\" if count_procedure > 0 else '0 (0.00%)'\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def add_TOTCHG_table_3(table):\n",
    "    local_df = procedure_df.copy()  # Work on a copy\n",
    "    local_df['TOTCHG_cat'] = (local_df['TOTCHG'] > avg_totchg).astype(int)\n",
    "\n",
    "    for category in [0, 1]:\n",
    "        subset_df = local_df[local_df['TOTCHG_cat'] == category]\n",
    "        count_procedure = subset_df.shape[0]\n",
    "        count_complications = subset_df['complications'].sum()\n",
    "\n",
    "        table.loc[f'TOTCHG_{category}', 'procedure'] = f\"{count_procedure} ({count_procedure/len(local_df):.2%})\"\n",
    "        table.loc[f'TOTCHG_{category}', 'complication'] = f\"{count_complications} ({count_complications/count_procedure:.2%})\" if count_procedure > 0 else '0 (0.00%)'\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def add_complication_summary(table, procedure_df):\n",
    "    \"\"\"\n",
    "    Add complication summary statistics to a table.\n",
    "\n",
    "    Parameters:\n",
    "    - table (pd.DataFrame): The table to which the summary statistics will be added.\n",
    "    - procedure_df (pd.DataFrame): The DataFrame containing procedure data, including complications.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated table with complication summary statistics.\n",
    "    \"\"\"\n",
    "    total_complications = procedure_df['complications'].sum()\n",
    "    table.loc['total count', 'complication'] = f\"{total_complications} ({total_complications/len(procedure_df):.2%})\"\n",
    "    complication_age_mean = procedure_df[procedure_df['complications'] == 1]['AGE'].mean()\n",
    "    complication_age_std = procedure_df[procedure_df['complications'] == 1]['AGE'].std()\n",
    "    table.loc['AGE (mean, std)', 'complication'] = f\"{complication_age_mean:.2f}, {complication_age_std:.2f}\"\n",
    "    return table\n",
    "\n",
    "def add_patient_characteristics(table, procedure_df, characteristics):\n",
    "    \"\"\"\n",
    "    Add patient characteristics to a table.\n",
    "\n",
    "    Parameters:\n",
    "    - table (pd.DataFrame): The table to which the patient characteristics will be added.\n",
    "    - procedure_df (pd.DataFrame): The DataFrame containing procedure data.\n",
    "    - characteristics (list): A list of characteristics to be included.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated table with patient characteristics.\n",
    "    \"\"\"\n",
    "    characteristics = [col for col in patient_columns[1:] if col not in condensed] # non-condensed variables\n",
    "    \n",
    "    for col in characteristics:\n",
    "        for value in procedure_df[col].unique():\n",
    "            count_procedure = procedure_df[procedure_df[col] == value].shape[0]\n",
    "            if count_procedure > 0:\n",
    "                table.loc[f'{col}_{value}', 'procedure'] = f\"{count_procedure} ({count_procedure/len(procedure_df):.2%})\"\n",
    "    return table\n",
    "\n",
    "def add_complication_data(table, procedure_df, characteristics):\n",
    "    \"\"\"\n",
    "    Add complication data to a table.\n",
    "\n",
    "    Parameters:\n",
    "    - table (pd.DataFrame): The table to which the complication data will be added.\n",
    "    - procedure_df (pd.DataFrame): The DataFrame containing procedure data, including complications.\n",
    "    - characteristics (list): A list of characteristics to consider for complication data.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated table with complication data.\n",
    "    \"\"\"\n",
    "    for col in characteristics:\n",
    "        complication_counts = procedure_df.groupby(col)['complications'].sum()\n",
    "        total_counts = procedure_df[col].value_counts()\n",
    "        complication_proportion = complication_counts / total_counts\n",
    "\n",
    "        for value, count in complication_counts.items():\n",
    "            if total_counts.get(value, 0) > 0:\n",
    "                proportion = complication_proportion.get(value, 0)\n",
    "                table.loc[f'{col}_{value}', 'complication'] = f\"{count} ({proportion:.2%})\"\n",
    "    return table\n",
    "\n",
    "def get_table_3_counts(procedure_df, condensed):\n",
    "    \"\"\"\n",
    "    Create a table with patient characteristics and complication information.\n",
    "\n",
    "    Parameters:\n",
    "    - procedure_df (pd.DataFrame): The DataFrame containing procedure data, including complications.\n",
    "    - condensed (list): A list of condensed variables to consider.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The table containing patient characteristics and complication information.\n",
    "    \"\"\"\n",
    "    table = pd.DataFrame(columns=['procedure', 'complication']).astype('object')\n",
    "    table.loc['total count', 'procedure'] = f\"{len(procedure_df)} ({len(procedure_df)/len(procedure_df):.2%})\"\n",
    "    age_mean_std_procedure = f\"{procedure_df['AGE'].mean():.2f}, {procedure_df['AGE'].std():.2f}\"\n",
    "    table.loc['AGE (mean, std)', 'procedure'] = age_mean_std_procedure\n",
    "    \n",
    "    characteristics = [col for col in patient_columns[1:] if col not in condensed] # non-condensed variables\n",
    "\n",
    "    table = add_complication_summary(table, procedure_df, )\n",
    "    table = add_patient_characteristics(table, procedure_df, characteristics)\n",
    "    table = add_complication_data(table, procedure_df, characteristics)\n",
    "\n",
    "    for condensed_variable in condensed:\n",
    "        if condensed_variable == 'TOTCHG':\n",
    "            table = add_TOTCHG_table_3(table)\n",
    "        elif condensed_variable == 'LOS':\n",
    "            table = add_LOS_table_3(table)\n",
    "        elif condensed_variable == 'PRDAY1':\n",
    "            table = add_PRDAY1_table_3(table)\n",
    "        elif condensed_variable == 'AGE':\n",
    "            table = add_AGE_table_3(table)\n",
    "\n",
    "    # Split the table into the first row and the rest\n",
    "    first_row = table.iloc[:2]\n",
    "    rest_of_table = table.iloc[2:]\n",
    "\n",
    "    # Sort the rest of the table based on the index (row labels) in ascending order\n",
    "    rest_of_table_sorted = rest_of_table.copy().sort_index()\n",
    "\n",
    "    # Concatenate the first row with the sorted rest of the table\n",
    "    sorted_table = pd.concat([first_row, rest_of_table_sorted])\n",
    "\n",
    "    return sorted_table\n",
    "\n",
    "    \n",
    "table_3_counts = get_table_3_counts(procedure_df, condensed)\n",
    "table_3_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2002c733-f4a8-49c9-95a2-4dddf37a1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_unchanged(procedure_df_snapshot, procedure_df)\n",
    "assert_unchanged(diagnosis_df_snapshot, diagnosis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a155d74e-ddf2-4a52-9633-59ccfaa6b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condense_LOS(df):\n",
    "    # Create a copy of the DataFrame to avoid modifying the original DataFrame\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Define the bins and labels for the LOS\n",
    "    bins = [0, 1, 2, 3, 4, 5, 6, 7, float('inf')]\n",
    "    labels = ['0', '1', '2', '3', '4', '5', '6', '7+']  # Including '0' as a category for LOS value of 0\n",
    "\n",
    "    # Apply the pd.cut function to categorize LOS\n",
    "    df_copy['LOS'] = pd.cut(df_copy['LOS'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # Handle NaN values if necessary\n",
    "    # df_copy['LOS_cat'] = df_copy['LOS_cat'].cat.add_categories('NaN').fillna('NaN')\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "def condense_PRDAY1(df):\n",
    "    def categorize_prday1(value):\n",
    "        if value >= -4 and value <= -1:\n",
    "            return 'before' # before admission \n",
    "        elif value == 0:\n",
    "            return 'during' # during admission\n",
    "        elif value >= 1:\n",
    "            return 'after' # after admission\n",
    "        else:\n",
    "            return 'other' # other \n",
    "\n",
    "    df.loc[:, 'PRDAY1'] = df['PRDAY1'].apply(categorize_prday1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def condense_TOTCHG(df):\n",
    "    #avg_totchg = round(diagnosis_df['TOTCHG'].mean())\n",
    "    df.loc[:, 'TOTCHG'] = (df['TOTCHG'] > avg_totchg).astype(int)\n",
    "    return df\n",
    "\n",
    "#def condense_AGE(df):\n",
    " #   df.loc[:, 'AGE'] = (df['AGE'] > avg_age).astype(int)\n",
    "  #  return df\n",
    "\n",
    "def condense_AGE(df):\n",
    "    # Convert AGE to numeric, handling non-numeric entries as NaN\n",
    "    df['AGE'] = pd.to_numeric(df['AGE'], errors='coerce')\n",
    "\n",
    "    # Define adjusted age bins and labels\n",
    "    bins = [35,50, 75, float('inf')]\n",
    "    labels = ['35-49','50-74', '75+']\n",
    "    df['AGE'] = pd.cut(df['AGE'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_odd_ratios_table_3():\n",
    "    or_df = procedure_df[demographic_independent_variables+['complications']]\n",
    "    or_df = condense_LOS(or_df)\n",
    "    or_df = condense_PRDAY1(or_df)\n",
    "    or_df = condense_TOTCHG(or_df)\n",
    "    or_df = condense_AGE(or_df)\n",
    "    old_len = len(or_df)\n",
    "    or_df = or_df.dropna()\n",
    "    new_len = len(or_df)\n",
    "    print(f\"dropped {old_len - new_len} discharges with missing data\")\n",
    "\n",
    "    or_df = pd.get_dummies(or_df, columns=or_df.columns.drop('complications'))\n",
    "\n",
    "    # Sort the columns in alphabetical order while keeping 'complications' as the first column\n",
    "    sorted_columns = sorted(or_df.columns.drop('complications'))\n",
    "    or_df = or_df[['complications'] + sorted_columns]\n",
    "    \n",
    "    return or_df\n",
    "\n",
    "or_df3 = get_odd_ratios_table_3()\n",
    "\n",
    "\n",
    "def get_demographic_reference_variables(df):\n",
    "    # Check if 'demographic_reference_variables' exists globally\n",
    "    if 'demographic_reference_variables' in globals():\n",
    "        return globals()['reference_variables']\n",
    "\n",
    "    # Identify all unique category prefixes (considering the part before the last underscore)\n",
    "    unique_categories = set('_'.join(col.split('_')[:-1]) for col in df.columns if '_' in col)\n",
    "\n",
    "    # Initialize a dictionary to store reference variables for each category\n",
    "    reference_variables = {}\n",
    "\n",
    "    for category in unique_categories:\n",
    "        # Find the first dummy variable of each category to use as the reference\n",
    "        reference_var = next((col for col in df.columns if col.startswith(category + '_')), None)\n",
    "        \n",
    "        if reference_var:\n",
    "            reference_variables[category] = reference_var\n",
    "\n",
    "    return reference_variables\n",
    "\n",
    "def get_demographic_reference_variables(df):\n",
    "    # Check if 'demographic_reference_variables' exists globally\n",
    "    if 'demographic_reference_variables' in globals():\n",
    "        return globals()['reference_variables']\n",
    "\n",
    "    # Identify all unique category prefixes (considering the part before the last underscore)\n",
    "    unique_categories = set('_'.join(col.split('_')[:-1]) for col in df.columns if '_' in col)\n",
    "\n",
    "    # Initialize a dictionary to store reference variables for each category\n",
    "    reference_variables = {}\n",
    "\n",
    "    for category in unique_categories:\n",
    "        if category == 'AGE':\n",
    "            # Explicitly set the reference variable for AGE \n",
    "            reference_var = f'{category}_35-49'\n",
    "            #reference_var = f'{category}_65+'\n",
    "        else:\n",
    "            # For other categories, find the first dummy variable as the reference\n",
    "            reference_var = next((col for col in df.columns if col.startswith(category + '_')), None)\n",
    "        \n",
    "        if reference_var:\n",
    "            reference_variables[category] = reference_var\n",
    "\n",
    "    return reference_variables\n",
    "\n",
    "reference_vars = get_demographic_reference_variables(or_df3)\n",
    "print(\"Reference variables for each category:\")\n",
    "for category, ref_var in reference_vars.items():\n",
    "    print(f\"{category}: {ref_var}\")\n",
    "\n",
    "or_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7097f35f-c201-4ffc-8e86-e19399adfc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_columns(df):\n",
    "    all_numeric = True\n",
    "    no_missing_values = True\n",
    "    problematic_columns = []\n",
    "\n",
    "    # Check each column for data type and missing values\n",
    "    for col in df.columns:\n",
    "        dtype = df[col].dtype\n",
    "        missing_values = df[col].isnull().sum()\n",
    "\n",
    "        if dtype == 'object' or missing_values > 0:\n",
    "            # If any column is non-numeric or has missing values, record this\n",
    "            all_numeric = False if dtype == 'object' else all_numeric\n",
    "            no_missing_values = False if missing_values > 0 else no_missing_values\n",
    "\n",
    "            # Store information about problematic columns\n",
    "            dtype_info = \"Not Numeric\" if dtype == 'object' else \"Numeric\"\n",
    "            missing_info = f\"Missing Values: {missing_values}\" if missing_values > 0 else \"No Missing Values\"\n",
    "            problematic_columns.append(f\"Column: {col}, Type: {dtype_info}, {missing_info}\")\n",
    "\n",
    "    # Print statements based on the checks\n",
    "    if all_numeric and no_missing_values:\n",
    "        print(\"All types numeric, no missing values\")\n",
    "    else:\n",
    "        for info in problematic_columns:\n",
    "            print(info)\n",
    "\n",
    "check_columns(or_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e1ad61-74cb-47e6-acaf-a6701c874a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_3_logistic_regression(df, dependent_var, independent_vars_prefixes, reference_vars):\n",
    "    results = {}\n",
    "    for prefix in independent_vars_prefixes:\n",
    "        try:\n",
    "            # Filter variables with the same prefix, excluding the reference variable\n",
    "            vars_with_prefix = [col for col in df.columns if col.startswith(prefix) and col != reference_vars[prefix]]\n",
    "\n",
    "            # Define X and y\n",
    "            X = df[vars_with_prefix].astype(float)\n",
    "            y = df[dependent_var].astype(float)\n",
    "\n",
    "            # Add constant to the model\n",
    "            X = sm.add_constant(X)\n",
    "\n",
    "            # Fit logistic regression model\n",
    "            model = sm.Logit(y, X)\n",
    "            fitted_model = model.fit()\n",
    "\n",
    "            # Calculating Odds Ratios, Confidence Intervals, and P-values\n",
    "            for var in vars_with_prefix:\n",
    "                odds_ratio = np.exp(fitted_model.params[var]).round(2)\n",
    "                conf = fitted_model.conf_int().loc[var]\n",
    "                ci_lower, ci_upper = np.exp(conf[0]).round(2), np.exp(conf[1]).round(2)\n",
    "                p_value = fitted_model.pvalues[var].round(3)\n",
    "\n",
    "                # Store results\n",
    "                results[var] = {'OR': odds_ratio, '2.5%': ci_lower, '97.5%': ci_upper, 'p-value': p_value}\n",
    "        except Exception as e:\n",
    "            print(f\"Error with variable prefix {prefix}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "\n",
    "logistic_regression_results_3 = get_table_3_logistic_regression(or_df3, \n",
    "                                                                dependent_var, # globally defined \n",
    "                                                                demographic_independent_variables, # globally_defined\n",
    "                                                                reference_vars)\n",
    "\n",
    "# Transform the DataFrame\n",
    "logistic_regression_results_3['OR (95% CI)'] = logistic_regression_results_3.apply(\n",
    "    lambda row: f\"{row['OR']} ({row['2.5%']} - {row['97.5%']})\", axis=1\n",
    ")\n",
    "logistic_regression_results_3 = logistic_regression_results_3[['OR (95% CI)', 'p-value']]\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "logistic_regression_results_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72678340-5c3f-49a0-9b58-90b0df29a561",
   "metadata": {},
   "source": [
    "Single Multivariable Logistic Regression Model:\n",
    "\n",
    "Approach: Fit one logistic regression model including all demographic variables as independent variables. This model assesses the effect of each variable on the outcome while controlling for the effects of the other variables.\n",
    "Odds Ratios: The ORs obtained from this model reflect the effect of each demographic variable on the outcome, adjusted for the influence of other variables in the model. This is important if there are potential confounders or if the variables are correlated.\n",
    "Interpretation: This approach provides a more holistic understanding of how each variable contributes to the outcome in the presence of other variables. It's more appropriate when the goal is to understand the effect of each variable in a real-world, multivariable context.\n",
    "\n",
    "In summary, for Table 3, if the aim is to understand the independent contribution of each demographic variable to the risk of complications, considering the potential interplay and confounding effects of other variables, a single multivariable logistic regression model would be more appropriate. This approach provides adjusted odds ratios that offer a more comprehensive view of each variable's effect in the context of others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b904b3-b254-45c3-adab-0f1ca999ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_unchanged(procedure_df_snapshot, procedure_df)\n",
    "assert_unchanged(diagnosis_df_snapshot, diagnosis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00254312-6ffc-44e9-b4af-f1be0bb2cccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Table_3():\n",
    "\n",
    "    # Convert the index of logistic_regression_results to a column if it's not already\n",
    "    logistic_regression_results_3.reset_index(inplace=True)\n",
    "    logistic_regression_results_3.rename(columns={'index': 'variable'}, inplace=True)\n",
    "\n",
    "    # Merge the tables\n",
    "    # This assumes that the rows in table_3_counts are named exactly as the variables in logistic_regression_results\n",
    "    table_3 = table_3_counts.merge(logistic_regression_results_3, left_index=True, right_on='variable', how='left')\n",
    "\n",
    "    # If needed, set 'variable' column as the new index\n",
    "    table_3.set_index('variable', inplace=True)\n",
    "    return table_3\n",
    "\n",
    "table_3 = get_Table_3()\n",
    "table_3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e89070-b38f-4001-b6b3-d3fae951c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_3.to_csv(f\"{data_folder_name}/{researcher}_{table_3_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce48c446-f217-437f-a82d-3967f18e4691",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Table 4: Medical comorbidities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b4bb37-e5d2-4f24-9751-d35a357c0ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_unchanged(procedure_df_snapshot, procedure_df)\n",
    "assert_unchanged(diagnosis_df_snapshot, diagnosis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a890dd7f-020b-408b-8852-8dc80e75119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_4_counts(procedure_df):\n",
    "    # Initialize a list to store the results\n",
    "    results = []\n",
    "\n",
    "    # Function to format count and proportion\n",
    "    def format_count_proportion(count, total):\n",
    "        proportion = (count / total * 100) if total != 0 else 0\n",
    "        return f\"{count} ({proportion:.2f}%)\"\n",
    "\n",
    "    # Iterate over each comorbidity column\n",
    "    for col in comorbidity_columns:\n",
    "        # Calculate the counts for the comorbidity\n",
    "        total_count = procedure_df[col].value_counts()\n",
    "\n",
    "        # Calculate the counts for complications within each comorbidity group\n",
    "        complication_count = procedure_df[procedure_df['complications'] == 1][col].value_counts()\n",
    "\n",
    "        # Store the results for comorbidity presence (1) and absence (0)\n",
    "        for value in [0, 1]:\n",
    "            total = total_count.get(value, 0)\n",
    "            comp_count = complication_count.get(value, 0)\n",
    "\n",
    "            results.append({\n",
    "                'Comorbidity': f\"{col}_{value}\",\n",
    "                'Patients': f\"{total} ({total / len(procedure_df) * 100:.2f}%)\",\n",
    "                'Patients w/ Complication': format_count_proportion(comp_count, total)\n",
    "            })\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Setting 'Comorbidity' as the index\n",
    "    results_df.set_index('Comorbidity', inplace=True)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "table_4_counts = get_table_4_counts(procedure_df)\n",
    "table_4_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab27b86-d304-4765-a2a7-431ec4794e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_odd_ratios_table_4():\n",
    "    or_df4 = procedure_df[comorbidity_columns + ['complications'] + table_4_adjusted]\n",
    "    or_df4 = condense_AGE(or_df4) \n",
    "\n",
    "    # Number of rows before dropping NaNs\n",
    "    rows_before = or_df4.shape[0]\n",
    "    # Drop rows with NaN values\n",
    "    or_df4 = or_df4.dropna()\n",
    "    # Number of rows after dropping NaNs\n",
    "    rows_after = or_df4.shape[0]\n",
    "\n",
    "    # Calculate and print the number of rows dropped\n",
    "    rows_dropped = rows_before - rows_after\n",
    "    print(f\"Number of discharges dropped: {rows_dropped}\")\n",
    "\n",
    "    # Convert only comorbidity_columns to dummy variables\n",
    "    or_df4 = pd.get_dummies(or_df4, columns=comorbidity_columns + table_4_adjusted)\n",
    "    return or_df4\n",
    "\n",
    "or_df4 = get_odd_ratios_table_4()\n",
    "or_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967828c-45a1-4194-81d2-9e7ca270ba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covariate_columns(or_df):\n",
    "    covariate_columns = [col for col in or_df.columns if col.startswith(tuple([prefix + '_' for prefix in table_4_adjusted]))]\n",
    "\n",
    "    # Extracting the unique prefixes (like 'FEMALE', 'AGE', 'RACE')\n",
    "    prefixes = set(col.split('_')[0] for col in covariate_columns)\n",
    "\n",
    "    # Finding the first instance of each prefix\n",
    "    first_instances = {prefix: next(col for col in covariate_columns if col.startswith(prefix)) for prefix in prefixes}\n",
    "    \n",
    "    # Hardcoding the replacement for the 'AGE' prefix\n",
    "    first_instances['AGE'] = 'AGE_35-49'\n",
    "\n",
    "    # Exclude the first instances to get the adjusted covariates\n",
    "    adjusted_covariates = [col for col in covariate_columns if col not in first_instances.values()]\n",
    "\n",
    "    return adjusted_covariates\n",
    "\n",
    "covariate_columns_table_4 = get_covariate_columns(or_df4)\n",
    "covariate_columns_table_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff7cab-320e-4e24-a314-a3ae3f12e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variables_to_exclude(df, threshold):\n",
    "    exclude_dict = {}\n",
    "    for col in df.columns:\n",
    "        true_count = df[col].sum()\n",
    "        if true_count < threshold:\n",
    "            exclude_dict[col] = true_count\n",
    "    return exclude_dict\n",
    "\n",
    "# Example usage\n",
    "threshold = 11  # Set your threshold here\n",
    "variables_to_exclude_table_4 = get_variables_to_exclude(or_df4, threshold)\n",
    "\n",
    "print(f\"excluding: {variables_to_exclude_table_4}\")\n",
    "print(\"______________________________________\")\n",
    "\n",
    "# todo: exlude \"perfect prediction\" or \"complete separation\" variables such as CMR_CANCER_LYMPH where 0 complications exist\n",
    "      \n",
    "def calculate_logistic_regression_with_reference(df, dependent_var, independent_vars_prefixes, covariates, variables_to_exclude):\n",
    "    results = {}\n",
    "\n",
    "    for prefix in independent_vars_prefixes:\n",
    "        reference_var = f\"{prefix}_0\"\n",
    "        target_var = f\"{prefix}_1\"\n",
    "\n",
    "        # Check if the target variable is in the exclusion list\n",
    "        if target_var not in variables_to_exclude:\n",
    "            try:\n",
    "                # Define X and y\n",
    "                X = df[[target_var] + covariates].astype(float)\n",
    "                y = df[dependent_var].astype(float)\n",
    "\n",
    "                X = sm.add_constant(X)\n",
    "                model = sm.Logit(y, X)\n",
    "                fitted_model = model.fit(maxiter=1000)\n",
    "\n",
    "                odds_ratio = np.exp(fitted_model.params[target_var]).round(2)\n",
    "                conf = fitted_model.conf_int().loc[target_var]\n",
    "                ci_lower, ci_upper = np.exp(conf[0]).round(2), np.exp(conf[1]).round(2)\n",
    "                p_value = fitted_model.pvalues[target_var].round(3)\n",
    "\n",
    "                results[f\"{prefix}_1\"] = {'OR': odds_ratio, '2.5%': ci_lower, '97.5%': ci_upper, 'p-value': p_value}\n",
    "            except Exception as e:\n",
    "                print(f\"Error with variable {prefix}: {e}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    results_df.index.name = 'Comorbidity'\n",
    "\n",
    "    results_df['OR (95% CI)'] = results_df.apply(\n",
    "        lambda row: f\"{row['OR']} ({row['2.5%']} - {row['97.5%']})\", axis=1\n",
    "    )\n",
    "    final_df = results_df[['OR (95% CI)', 'p-value']]\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "#variables_to_exclude = get_variables_to_exclude(or_df4, 2)  # Assume this is your exclusion list\n",
    "logistic_regression_results_table_4 = calculate_logistic_regression_with_reference(or_df4, \n",
    "                                                                           dependent_var, \n",
    "                                                                           comorbidity_columns, \n",
    "                                                                           covariate_columns_table_4, \n",
    "                                                                           variables_to_exclude_table_4)\n",
    "logistic_regression_results_table_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05b2ec1-b0f3-4663-89a8-979d6adfa908",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_4 = pd.merge(table_4_counts, logistic_regression_results_table_4, on='Comorbidity', how='left')\n",
    "table_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ac4dc4-c07c-4877-b6fd-4f0d45325b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_4.to_csv(f\"{data_folder_name}/{researcher}_{table_4_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9392f23-4498-44bb-8a73-abb2ea3868a0",
   "metadata": {},
   "source": [
    "# Table 5: Surgical parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14354887-5a1d-446d-9b47-e9bd9b29ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_surgical_params(df):\n",
    "    # Helper function to check if a code starts with any prefix in a list\n",
    "    def starts_with_any(code, prefixes):\n",
    "        return any(code.startswith(prefix) for prefix in prefixes) if code is not None else False\n",
    "\n",
    "    # List of procedure columns to check\n",
    "    procedure_columns = [f'I10_PR{n}' for n in range(1, 25)]\n",
    "\n",
    "    # Function to check across multiple columns\n",
    "    def check_procedures(row, prefixes):\n",
    "        return any(starts_with_any(row[col], prefixes) for col in procedure_columns)\n",
    "\n",
    "    # Independent variables based on new surgical parameters\n",
    "    df['bilateral'] = df.apply(lambda row: check_procedures(row, bilateral), axis=1).astype(int)\n",
    "    df['unicondylar'] = df.apply(lambda row: check_procedures(row, unicondylar), axis=1).astype(int)\n",
    "    df['patellofemoral'] = df.apply(lambda row: check_procedures(row, patellofemoral), axis=1).astype(int)\n",
    "    \n",
    "    df['cemented'] = df.apply(lambda row: check_procedures(row, cemented), axis=1).astype(int)\n",
    "    df['uncemented'] = df.apply(lambda row: check_procedures(row, uncemented), axis=1).astype(int)\n",
    "    \n",
    "    df['synthetic'] = df.apply(lambda row: check_procedures(row, synthetic), axis=1).astype(int)\n",
    "    df['autologous'] = df.apply(lambda row: check_procedures(row, autologous), axis=1).astype(int)\n",
    "    df['non_autologous'] = df.apply(lambda row: check_procedures(row, non_autologous), axis=1).astype(int)\n",
    "    \n",
    "    df['articulating_spacer_yes'] = df.apply(lambda row: check_procedures(row, articulating_spacer_yes), axis=1).astype(int)\n",
    "\n",
    "    # Covariates:\n",
    "    df['comorbidity'] = df[comorbidity_columns].any(axis=1).astype(int)  # create dummy variable representing all comorbidities\n",
    "    df = condense_AGE(df)  # convert 'AGE' into a dummy variable\n",
    "    \n",
    "    return df \n",
    "\n",
    "procedure_df = add_surgical_params(procedure_df)\n",
    "\n",
    "procedure_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc7068-297e-4c0a-9fb4-939d042deeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_surgical_partitions(procedure_df, partitions):\n",
    "    # Extract the specified columns into a separate DataFrame\n",
    "    subset_df = procedure_df[partitions]\n",
    "\n",
    "    # Count the number of rows where the sum of all columns is not 1\n",
    "    non_partition_count = (subset_df.sum(axis=1) != 1).sum()\n",
    "    \n",
    "    portion = 100 * round(non_partition_count / len(subset_df), 4)\n",
    "\n",
    "    print(f\"Number of rows where columns do not form a partition: {non_partition_count} ({portion}%)\")\n",
    "    return subset_df[(subset_df.sum(axis=1) != 1)]\n",
    "\n",
    "# Example usage\n",
    "check_surgical_partitions(procedure_df, ['bilateral', 'unicondylar', 'patellofemoral'])\n",
    "check_surgical_partitions(procedure_df, ['cemented', 'uncemented'])\n",
    "check_surgical_partitions(procedure_df, ['synthetic', 'autologous', 'non_autologous'])\n",
    "check_surgical_partitions(procedure_df, ['articulating_spacer_yes'])  # For binary, only one column needed"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02146411-cfdb-476e-840b-ca2ff59e6831",
   "metadata": {},
   "source": [
    "def count_prefix_occurrences(df, column, prefixes):\n",
    "    # Create a mask to filter rows where the column starts with any of the prefixes\n",
    "    mask = df[column].astype(str).isin(prefixes)\n",
    "\n",
    "    # Count the number of rows that match the mask\n",
    "    count = mask.sum()\n",
    "\n",
    "    print(f\"Number of rows where '{column}' starts with one of the specified prefixes: {count}\")\n",
    "    return count\n",
    "\n",
    "\n",
    "count_prefix_occurrences(procedure_df, 'I10_PR1', bilateral)\n",
    "count_prefix_occurrences(procedure_df, 'I10_PR1', unicondylar)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3091fd0c-7b52-473a-840c-218005334601",
   "metadata": {},
   "source": [
    "post = procedure_df['I10_PR1'].apply(lambda x: any(x.startswith(prefix) for prefix in posterior_approach)).sum()\n",
    "ant = procedure_df['I10_PR1'].apply(lambda x: any(x.startswith(prefix) for prefix in anterior_approach)).sum()\n",
    "\n",
    "round(100 - (100*(post + ant) / len(procedure_df)), 2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "768f02d7-1eab-4800-b435-d8b4250f3da4",
   "metadata": {},
   "source": [
    "auto = procedure_df['I10_PR1'].apply(lambda x: any(x.startswith(prefix) for prefix in autologous_tissue_substitute)).sum()\n",
    "non_auto = procedure_df['I10_PR1'].apply(lambda x: any(x.startswith(prefix) for prefix in non_autologous_tissue_substitute)).sum()\n",
    "fusion = procedure_df['I10_PR1'].apply(lambda x: any(x.startswith(prefix) for prefix in interbody_fusion_device)).sum()\n",
    "\n",
    "round(100 - (100*(auto + non_auto + fusion) / len(procedure_df)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f29ce6d-7cd7-474d-92f6-884955332976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_patient_characteristics(table, procedure_df):\n",
    "    \"\"\"\n",
    "    Add patient characteristics to a table.\n",
    "\n",
    "    Parameters:\n",
    "    - table (pd.DataFrame): The table to which the patient characteristics will be added.\n",
    "    - procedure_df (pd.DataFrame): The DataFrame containing procedure data.\n",
    "    - characteristics (list): A list of characteristics to be included.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated table with patient characteristics.\n",
    "    \"\"\"\n",
    "    \n",
    "    non_condensed_vars = [col for col in surgical_independent_variables_with_reference]\n",
    "    \n",
    "    \n",
    "    for col in non_condensed_vars:\n",
    "        for value in procedure_df[col].unique():\n",
    "            count_procedure = procedure_df[procedure_df[col] == value].shape[0]\n",
    "            if count_procedure > 0:\n",
    "                table.loc[f'{col}_{value}', 'procedure'] = f\"{count_procedure} ({count_procedure/len(procedure_df):.2%})\"\n",
    "    return table\n",
    "\n",
    "def add_complication_data(table, procedure_df):\n",
    "    \"\"\"\n",
    "    Add complication data to a table.\n",
    "\n",
    "    Parameters:\n",
    "    - table (pd.DataFrame): The table to which the complication data will be added.\n",
    "    - procedure_df (pd.DataFrame): The DataFrame containing procedure data, including complications.\n",
    "    - characteristics (list): A list of characteristics to consider for complication data.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The updated table with complication data.\n",
    "    \"\"\"\n",
    "    non_condensed_vars = [col for col in surgical_independent_variables_with_reference]\n",
    "\n",
    "    for col in non_condensed_vars:\n",
    "        complication_counts = procedure_df.groupby(col)['complications'].sum()\n",
    "        total_counts = procedure_df[col].value_counts()\n",
    "        complication_proportion = complication_counts / total_counts\n",
    "\n",
    "        for value, count in complication_counts.items():\n",
    "            if total_counts.get(value, 0) > 0:\n",
    "                proportion = complication_proportion.get(value, 0)\n",
    "                table.loc[f'{col}_{value}', 'complication'] = f\"{count} ({proportion:.2%})\"\n",
    "    return table\n",
    "\n",
    "def get_table_5_counts(procedure_df, ):\n",
    "    \"\"\"\n",
    "    Create a table with patient characteristics and complication information.\n",
    "\n",
    "    Parameters:\n",
    "    - procedure_df (pd.DataFrame): The DataFrame containing procedure data, including complications.\n",
    "    - condensed (list): A list of condensed variables to consider.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The table containing patient characteristics and complication information.\n",
    "    \"\"\"\n",
    "    table = pd.DataFrame(columns=['procedure', 'complication']).astype('object')\n",
    "    #table.loc['total count', 'procedure'] = f\"{len(procedure_df)} ({len(procedure_df)/len(procedure_df):.2%})\"\n",
    "    #age_mean_std_procedure = f\"{procedure_df['AGE'].mean():.2f}, {procedure_df['AGE'].std():.2f}\"\n",
    "    #table.loc['AGE (mean, std)', 'procedure'] = age_mean_std_procedure\n",
    "    \n",
    "\n",
    "    #table = add_complication_summary(table, procedure_df)\n",
    "    table = add_patient_characteristics(table, procedure_df)\n",
    "    table = add_complication_data(table, procedure_df)\n",
    "\n",
    "    #for condensed_variable in condensed_table_5:\n",
    "     #   if condensed_variable == 'AGE':\n",
    "      #      table = add_AGE_table_3(table)\n",
    "\n",
    "    # Split the table into the first row and the rest\n",
    "    first_row = table.iloc[:2]\n",
    "    rest_of_table = table.iloc[2:]\n",
    "\n",
    "    # Sort the rest of the table based on the index (row labels) in ascending order\n",
    "    rest_of_table_sorted = rest_of_table.copy().sort_index()\n",
    "\n",
    "    # Concatenate the first row with the sorted rest of the table\n",
    "    sorted_table = pd.concat([first_row, rest_of_table_sorted])\n",
    "\n",
    "    return sorted_table\n",
    "\n",
    "    \n",
    "table_5_counts = get_table_5_counts(procedure_df)\n",
    "table_5_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea1d0fc-c43f-43dd-b18c-544f950ad462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_odd_ratios_table_5():\n",
    "\n",
    "    or_df5 = procedure_df.copy() # already has independent vars + covariate vars (condensed age, comorbidity)\n",
    "\n",
    "    # Create dummy variables for each surgical parameter:\n",
    "\n",
    "    # or_df5 = add_surgical_params(or_df5)\n",
    "    \n",
    "    surg = [f\"{dependent_var}\"]+surgical_independent_variables+table_5_adjusted # relevant columns logistic regression \n",
    "    or_df5 = or_df5[surg]\n",
    "    \n",
    "    or_df5 = pd.get_dummies(or_df5, columns=or_df5.columns.drop('complications')) \n",
    "    \n",
    "    return or_df5\n",
    "\n",
    "or_df5 = get_odd_ratios_table_5()\n",
    "or_df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e4fe77-e9c2-4d83-b320-e0420348e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reference_variables(df, refrence_variables): \n",
    "    # Check if 'refrence_variables' exists globally\n",
    "    if f'{refrence_variables}' in globals():\n",
    "        return globals()[f'{refrence_variables}']\n",
    "\n",
    "    # Identify all unique category prefixes (considering the part before the last underscore)\n",
    "    unique_categories = set('_'.join(col.split('_')[:-1]) for col in df.columns if '_' in col)\n",
    "\n",
    "    # Initialize a dictionary to store reference variables for each category\n",
    "    reference_variables = {}\n",
    "\n",
    "    for category in unique_categories:\n",
    "        # Find the first dummy variable of each category to use as the reference\n",
    "        reference_var = next((col for col in df.columns if col.startswith(category + '_')), None)\n",
    "        \n",
    "        if reference_var:\n",
    "            reference_variables[category] = reference_var\n",
    "\n",
    "    return reference_variables\n",
    "\n",
    "def get_reference_variables(df, refrence_variables): \n",
    "    # Check if 'refrence_variables' exists globally\n",
    "    if f'{refrence_variables}' in globals():\n",
    "        return globals()[f'{refrence_variables}']\n",
    "\n",
    "    # Identify all unique category prefixes (considering the part before the last underscore)\n",
    "    unique_categories = set('_'.join(col.split('_')[:-1]) for col in df.columns if '_' in col)\n",
    "\n",
    "    # Initialize a dictionary to store reference variables for each category\n",
    "    reference_variables = {}\n",
    "\n",
    "    for category in unique_categories:\n",
    "        if category == 'AGE':\n",
    "            # Hardcoding the reference variable for 'AGE' \n",
    "            reference_variables[category] = 'AGE_35-49'\n",
    "        else:\n",
    "            # For other categories, find the first dummy variable as the reference\n",
    "            reference_var = next((col for col in df.columns if col.startswith(category + '_')), None)\n",
    "            \n",
    "            if reference_var:\n",
    "                reference_variables[category] = reference_var\n",
    "\n",
    "    return reference_variables\n",
    "\n",
    "\n",
    "reference_vars = get_reference_variables(or_df5, 'refrence_variables_table_5')\n",
    "print(\"Reference variables for each category:\")\n",
    "for category, ref_var in reference_vars.items():\n",
    "    print(f\"{category}: {ref_var}\") # needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7e37d3-d599-4779-9122-5c15c9a04476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covariate_columns(or_df, adjusted):\n",
    "    covariate_columns = [col for col in or_df.columns if col.startswith(tuple([prefix + '_' for prefix in adjusted]))]\n",
    "\n",
    "\n",
    "    # Extracting the unique prefixes (like 'FEMALE', 'AGE', 'RACE')\n",
    "    prefixes = set(col.split('_')[0] for col in covariate_columns)\n",
    "    \n",
    "\n",
    "    # Finding the first instance of each prefix\n",
    "    first_instances = {prefix: next(col for col in covariate_columns if col.startswith(prefix)) for prefix in prefixes}\n",
    "\n",
    "    # Exclude the first instances to get the adjusted covariates\n",
    "    adjusted_covariates = [col for col in covariate_columns if col not in first_instances.values()]\n",
    "\n",
    "    return adjusted_covariates\n",
    "\n",
    "covariate_columns_table_5 = get_covariate_columns(or_df5, table_5_adjusted)\n",
    "covariate_columns_table_5 # redundant (refactor get_reference_variables to include this logic) \n",
    "\n",
    "def get_covariate_columns(or_df, adjusted):\n",
    "    covariate_columns = [col for col in or_df.columns if col.startswith(tuple([prefix + '_' for prefix in adjusted]))]\n",
    "\n",
    "    # Extracting the unique prefixes (like 'FEMALE', 'AGE', 'RACE')\n",
    "    prefixes = set(col.split('_')[0] for col in covariate_columns)\n",
    "    \n",
    "    # Finding the first instance of each prefix\n",
    "    first_instances = {}\n",
    "    for prefix in prefixes:\n",
    "        if prefix == 'AGE':\n",
    "            # Hardcoding the reference variable for 'AGE'\n",
    "            first_instances[prefix] = 'AGE_35-49'\n",
    "        else:\n",
    "            first_instances[prefix] = next((col for col in covariate_columns if col.startswith(prefix)), None)\n",
    "\n",
    "    # Exclude the first instances to get the adjusted covariates\n",
    "    adjusted_covariates = [col for col in covariate_columns if col not in first_instances.values()]\n",
    "\n",
    "    return adjusted_covariates\n",
    "\n",
    "covariate_columns_table_5 = get_covariate_columns(or_df5, table_5_adjusted)\n",
    "covariate_columns_table_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc221df6-3955-45aa-9028-6b37b4c36f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variables_to_exclude(df, threshold):\n",
    "    exclude_dict = {}\n",
    "    for col in df.columns:\n",
    "        true_count = df[col].sum()\n",
    "        if true_count < threshold:\n",
    "            exclude_dict[col] = true_count\n",
    "    return exclude_dict\n",
    "\n",
    "# Example usage\n",
    "threshold = 11  # Set your threshold here\n",
    "variables_to_exclude_table_5 = get_variables_to_exclude(or_df5, threshold)\n",
    "\n",
    "print(f\"excluding: {variables_to_exclude_table_5}\")\n",
    "print(\"______________________________________\")\n",
    "\n",
    "# todo: exlude \"perfect prediction\" or \"complete separation\" variables such as CMR_CANCER_LYMPH where 0 complications exist\n",
    "      \n",
    "def calculate_logistic_regression_with_reference(df, dependent_var, independent_vars_prefixes, covariates, variables_to_exclude):\n",
    "    results = {}\n",
    "\n",
    "    for prefix in independent_vars_prefixes:\n",
    "        reference_var = f\"{prefix}_0\"\n",
    "        target_var = f\"{prefix}_1\"\n",
    "\n",
    "        # Check if the target variable is in the exclusion list\n",
    "        if target_var not in variables_to_exclude:\n",
    "            try:\n",
    "                # Define X and y\n",
    "                X = df[[target_var] + covariates].astype(float)\n",
    "                y = df[dependent_var].astype(float)\n",
    "\n",
    "                X = sm.add_constant(X)\n",
    "                model = sm.Logit(y, X)\n",
    "                fitted_model = model.fit(maxiter=1000)\n",
    "\n",
    "                odds_ratio = np.exp(fitted_model.params[target_var]).round(2)\n",
    "                conf = fitted_model.conf_int().loc[target_var]\n",
    "                ci_lower, ci_upper = np.exp(conf[0]).round(2), np.exp(conf[1]).round(2)\n",
    "                p_value = fitted_model.pvalues[target_var].round(3)\n",
    "\n",
    "                results[f\"{prefix}_1\"] = {'OR': odds_ratio, '2.5%': ci_lower, '97.5%': ci_upper, 'p-value': p_value}\n",
    "            except Exception as e:\n",
    "                print(f\"Error with variable {prefix}: {e}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    #results_df.index.name = 'Comorbidity'\n",
    "\n",
    "    results_df['OR (95% CI)'] = results_df.apply(\n",
    "        lambda row: f\"{row['OR']} ({row['2.5%']} - {row['97.5%']})\", axis=1\n",
    "    )\n",
    "    final_df = results_df[['OR (95% CI)', 'p-value']]\n",
    "\n",
    "    return final_df\n",
    "\n",
    "#variables_to_exclude = get_variables_to_exclude(or_df4, 2)  # Assume this is your exclusion list\n",
    "logistic_regression_results_table_5 = calculate_logistic_regression_with_reference(or_df5, \n",
    "                                                                           dependent_var, \n",
    "                                                                           surgical_independent_variables, \n",
    "                                                                           covariate_columns_table_5, \n",
    "                                                                           variables_to_exclude_table_5)\n",
    "logistic_regression_results_table_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed3a17-527c-4d5b-ba8d-c88f879ce657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table(counts_df, regression_df):\n",
    "\n",
    "    # Reset the index of logistic_regression_results_table_5 so that the index becomes a column\n",
    "    regression_df_reset = regression_df.reset_index()\n",
    "\n",
    "    # Perform the merge\n",
    "    # Note: Since table_5_counts is using index for merging, \n",
    "    # the resulting DataFrame will have the index from table_5_counts\n",
    "    table_5 = pd.merge(counts_df, regression_df_reset, \n",
    "                       left_index=True, right_on='index', how='left')\n",
    "\n",
    "    # Optionally, if you want to set 'index' column as the index of the merged DataFrame\n",
    "    table_5.set_index('index', inplace=True)\n",
    "    # Rename the index of table_5 to 'Parameter'\n",
    "    table_5.rename_axis('Parameter', inplace=True)\n",
    "\n",
    "    return table_5\n",
    "\n",
    "table_5 = get_table(table_5_counts, logistic_regression_results_table_5)\n",
    "table_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae1b8c4-9133-4b0f-a3d2-295cb65da9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_5.to_csv(f\"{data_folder_name}/{researcher}_{table_5_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed86897-8a9c-430a-902e-30755e37eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "# Convert seconds to minutes and seconds\n",
    "minutes = int(total_time // 60)\n",
    "seconds = int(total_time % 60)\n",
    "\n",
    "print(f\"Total execution time: {minutes} minutes, {seconds} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a25298-d273-41b9-ac0c-2bfd96e2d05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compartment\n",
    "base_bilateral = ['0SR.C06', '0SRC.07', '0SR.C0J', '0SR.C0K', '0SR.D06', '0SR.D07', '0SR.D0J', '0SR.D0K'] \n",
    "base_unicondylar = ['0SR.C0L', '0SR.C0M', '0SR.D0L', '0SR.D0M']\n",
    "base_patellofemoral = ['0SR.C0N', '0SR.D0N']\n",
    "\n",
    "# 2. Cementation status\n",
    "base_cemented = ['0SR.C069', '0SR.C0J9', '0SR.C0L9', '0SR.C0M9', '0SR.C0N9', '0SR.D069', '0SR.D0J9', '0SR.D0L9', '0SR.D0M9', '0SR.D0N9']\n",
    "base_uncemented = ['0SR.C06A', '0SR.C0JA', '0SR.C0LA', '0SR.C0MA', '0SR.C0NA', '0SR.D06A', '0SR.D0JA', '0SR.D0LA', '0SR.D0MA', '0SR.D0NA']\n",
    "\n",
    "# 3. Tissue substitute\n",
    "base_synthetic = ['0SR.C06', '0SR.C0J', '0SR.C0L', '0SR.C0M', '0SR.C0N', '0SR.D06', '0SR.D0J', '0SR.D0L', '0SR.D0M', '0SR.D0N']\n",
    "base_autologous = ['0SR.C07', '0SR.D07']\n",
    "base_non_autologous = ['0SR.C0K', '0SR.D0K']\n",
    "\n",
    "# 4. Articulating spacer\n",
    "base_articulating_spacer_yes = ['0SR.C0E', '0SR.D0E']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
